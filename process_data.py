import json
from route_plan import *
import pandas as pd
import numpy as np
import csv
from geopy.distance import geodesic
def know_data( column_names,csv_file):
    """
    è·å–æŒ‡å®šCSVæ–‡ä»¶ä¸­æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹å’Œæ•°æ®èŒƒå›´ã€‚

    å‚æ•°ï¼š
        csv_file (str): CSVæ–‡ä»¶çš„åç§°æˆ–è·¯å¾„ã€‚
        column_names (list): éœ€è¦åˆ†æçš„åˆ—ååˆ—è¡¨ã€‚

    è¿”å›ï¼š
        dict: åŒ…å«æ¯ä¸ªåˆ—çš„æ•°æ®ç±»å‹å’Œæ•°æ®èŒƒå›´çš„ä¿¡æ¯ã€‚
    """
    csv_file=f"available_data/{csv_file}.csv"
    data_range_string='data_range'

    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        if not column_names:
            column_names = df.columns.tolist()

        if isinstance(column_names, str):
            column_names = [column_names]
        # æ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in column_names if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ä»¥ä¸‹åˆ—åä¸å­˜åœ¨äºCSVæ–‡ä»¶ä¸­: {missing_columns}")

        result = {csv_file.replace(".csv",'').split('/')[1]:{}}

        for column_name in column_names:
            # è·å–åˆ—çš„æ•°æ®ç±»å‹
            column_data = df[column_name]
            data_type = column_data.dtypes

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸæ ¼å¼å­—ç¬¦ä¸²
            if data_type == 'object':
                try:
                    parsed_dates = pd.to_datetime(column_data, format='%Y-%m-%d', errors='coerce')
                    if parsed_dates.notna().all():
                        mapped_type = 'string(date)'
                        data_range = (
                            parsed_dates.min().strftime('%Y-%m-%d'),
                            parsed_dates.max().strftime('%Y-%m-%d')
                        )
                    else:
                        raise ValueError
                except ValueError:
                    mapped_type = 'string'
                    unique_values = column_data.dropna().unique()
                    if len(unique_values) <= 10:
                        data_range = list(unique_values)
                        data_range_string = 'All data type'

                    else:
                        sample_values = column_data.dropna().unique()[:3]
                        data_range = list(sample_values) if len(sample_values) > 0 else (None, None)
                        data_range_string = 'sampled three data'
            elif np.issubdtype(data_type, np.number):
                data_range_string = 'data_range'

                if np.issubdtype(data_type, np.integer):
                    mapped_type = 'int'
                else:
                    mapped_type = 'float'
                data_range = (column_data.min(), column_data.max())

            elif np.issubdtype(data_type, np.datetime64):
                data_range_string = 'data_range'

                mapped_type = 'string(date)'
                data_range = (
                    column_data.min().strftime('%Y-%m-%d') if pd.notnull(column_data.min()) else None,
                    column_data.max().strftime('%Y-%m-%d') if pd.notnull(column_data.max()) else None
                )
            else:
                mapped_type = 'æœªçŸ¥'
                sample_values = column_data.dropna().unique()[:3]
                data_range = list(sample_values) if len(sample_values) > 0 else (None, None)
                data_range_string='sampled three data'

            result[csv_file.replace(".csv",'').split('/')[1]][column_name] = {
                'data_type': mapped_type,
                data_range_string: data_range
            }

        print(result)
        return result

    except FileNotFoundError:
        return {"error": f"æ–‡ä»¶ '{csv_file}' æœªæ‰¾åˆ°ã€‚"}
    except ValueError as e:
        return {"error": str(e)}



def get_data(data_names, csv_filename):
    """
    ä»CSVæ–‡ä»¶ä¸­æŸ¥æ‰¾æ•°æ®åç§°å¹¶è¿”å›å¯¹åº”è¡Œæˆ–åˆ—çš„æ•°æ®ï¼Œè¿”å›æ ¼å¼ä¸ºJSONã€‚

    :param data_names: å•ä¸ªæ•°æ®åç§°æˆ–æ•°æ®åç§°åˆ—è¡¨
    :param csv_filename: CSVæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    :return: JSONæ ¼å¼çš„ç»“æœ
    """
    # ç¡®ä¿ data_names æ˜¯åˆ—è¡¨
    csv_filename = f"available_data/{csv_filename}.csv"

    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_filename, encoding='utf-8-sig')

        if isinstance(data_names, str):
            data_names = [data_names]
        if not data_names or len(data_names)==0:
            raw_data = df.to_dict(orient='records')
            data_intro = {"length": len(raw_data), "keys": list(df.columns)}
            print(data_intro)
            return raw_data
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰çš„ data_names éƒ½æ˜¯åˆ—å
        if all(name in df.columns for name in data_names):
            # å¦‚æœ data_names æ˜¯åˆ—åï¼Œè¿”å›æŒ‡å®šçš„åˆ—
            selected_columns = df[data_names]

            # æ£€æŸ¥æ˜¯å¦æœ‰ 'date' åˆ—
            if 'date' in df.columns:
                selected_columns['date'] = df['date']
            else:
                print("Warning: 'date' column not found in the CSV file.")

            # æ•°æ®ç®€ä»‹
            data_intro = {'length': len(selected_columns), 'keys': list(selected_columns.columns)}
            print(data_intro)

            # è¿”å›åŒ…å« date çš„ç»“æœ
            return selected_columns.to_dict(orient='records')

        # å¦‚æœ data_names ä¸ºç©ºï¼Œè¿”å›æ‰€æœ‰æ•°æ®


        # æŸ¥æ‰¾åŒ…å«æŒ‡å®šæ•°æ®åç§°çš„è¡Œ
        matched_rows = df[df.apply(lambda row: any(data_name in row.values for data_name in data_names), axis=1)]
        print(matched_rows)
        # å¦‚æœæœ‰åŒ¹é…çš„è¡Œï¼Œè¿”å›æ•°æ®å’ŒåŸºæœ¬ä¿¡æ¯
        if not matched_rows.empty:
            raw_data = matched_rows.to_dict(orient='records')
            data_intro = {'length': len(raw_data), 'keys': list(matched_rows.columns)}
            print(data_intro)
            return raw_data
        else:
            data_intro = {'length': 0}
            print(data_intro)
            return []

    except FileNotFoundError:
        return json.dumps({"error": "CSV file not found."}, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": str(e)}, ensure_ascii=False)
def get_parking(name,address,n):
    latitude, longitude=address
    """
    Find the nearest n locations to the given latitude and longitude.

    Parameters:
        file_path (str): Path to the CSV file containing 'latitude' and 'longitude' columns.
        latitude (float): Latitude of the input location.
        longitude (float): Longitude of the input location.
        n (int): Number of nearest locations to return.

    Returns:
        list: A JSON-compatible list of dictionaries representing the nearest n rows.
    """
    # Load the CSV file
    df = pd.read_csv('available_data/parking.csv')

    # Ensure the required columns exist
    if 'latitude' not in df.columns or 'longitude' not in df.columns:
        raise ValueError("The CSV file must contain 'latitude' and 'longitude' columns.")

    # Calculate distances
    def calculate_distance(row):
        return geodesic((latitude, longitude), (row['latitude'], row['longitude'])).kilometers

    df['distance'] = df.apply(calculate_distance, axis=1)

    # Sort by distance and select the top n rows
    nearest_locations = df.nsmallest(n, 'distance')

    result = [{
        "reference_point_name": name,
        "reference_point_location": {"latitude": latitude, "longitude": longitude},
        "nearest_locations": nearest_locations.to_dict(orient='records')
    }]

    return result



def plan_routes(start_longitude, start_latitude, end_longitude, end_latitude):
    best_3_routes = plan_routes_function(start_longitude, start_latitude, end_longitude, end_latitude, k=3)
    return best_3_routes
# get_data("Viby Bibliotek",'events')
# start_longitude, start_latitude = 10.21284, 56.16184  # åŒ—äº¬å¤©å®‰é—¨
# end_longitude, end_latitude = 10.164431,56.130402 # åŒ—äº¬æŸåœ°ç¤ºä¾‹
# #
# best_3_routes = plan_routes( start_longitude, start_latitude, end_longitude, end_latitude, k=3)
# # ä½¿ç”¨ç¤ºä¾‹
    # except Exception as e:
    #     return json.dumps({"error": str(e)}, ensure_ascii=False)

# # Step 1: æŸ¥è¯¢å¤©æ°”å¥½çš„æ•°æ®
# # æŸ¥è¯¢å¤©æ°”å¥½çš„æ•°æ®
# good_weather = get_data(['â˜€ï¸', 'ğŸŒ¤ï¸'], 'weather')
#
# # ç­›é€‰å¤©æ°”å¥½çš„æ—¥æœŸ
# good_weather_dates = [entry['date'] for entry in good_weather]
#
# # æŸ¥è¯¢æ±¡æŸ“ä¸ä¸¥é‡çš„æ•°æ®
# low_pollution = get_data(['Good'], 'pollution')
#
# # ç­›é€‰æ±¡æŸ“ä¸ä¸¥é‡çš„æ—¥æœŸ
# low_pollution_dates = [entry['date'] for entry in low_pollution]
#
# # æ‰¾åˆ°å¤©æ°”å¥½ä¸”æ±¡æŸ“ä¸ä¸¥é‡çš„å…¬å…±æ—¥æœŸ
# good_dates = list(set(good_weather_dates) & set(low_pollution_dates))
#
# # æŸ¥è¯¢è¿™äº›æ—¥æœŸå¯¹åº”çš„æ´»åŠ¨
# final_result = get_data(good_dates, 'events')
# print(final_result)
# # pollution=get_data('2014-08-01','pollution')
# wea=get_data(None,'pollution')
# # print(pollution[0])
# print(wea)
#
# get_data('Good', 'weather')
# get_data('Good', 'weather')
# # # print(know_data(final_result[0].keys(),'events'))
# library_location = get_data(['Tilst Bibliotek'], 'events')
# # Assuming the location details are in the first element of the list
# tilst_bibliotek_location = library_location[0]
#
# # Step 2: Use the latitude and longitude from Tilst Bibliotek to find parking nearby
# latitude = tilst_bibliotek_location['latitude']
# longitude = tilst_bibliotek_location['longitude']
#
# searched_result = get_parking('name',(latitude, longitude), 5)
# print(searched_result)